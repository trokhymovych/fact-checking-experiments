{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, LabelAccuracyEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "def _read_tsv(input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            if sys.version_info[0] == 2:\n",
    "                line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "            lines.append(line)\n",
    "        return lines\n",
    "    \n",
    "def _create_examples_snli(lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, line[0])\n",
    "        text_a = line[7]\n",
    "        text_b = line[8]\n",
    "        label = line[-1]\n",
    "        examples.append([guid, text_a, text_b, label])\n",
    "    return examples\n",
    "\n",
    "def _create_examples_mnli(lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, line[0])\n",
    "        text_a = line[8]\n",
    "        text_b = line[9]\n",
    "        label = line[-1]\n",
    "        examples.append([guid, text_a, text_b, label])\n",
    "    return examples\n",
    "\n",
    "train_snli = _create_examples_snli(_read_tsv('../SemBERT/glue_data/SNLI/train.tsv'), 'train_s')\n",
    "dev_snli = _create_examples_snli(_read_tsv('../SemBERT/glue_data/SNLI/dev.tsv'), 'test_s')\n",
    "test_snli = _create_examples_snli(_read_tsv('../SemBERT/glue_data/SNLI/test.tsv'), 'test_s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Train, Dev, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_nli_samples = []\n",
    "dev_nli_samples = []\n",
    "test_nli_samples = []\n",
    "\n",
    "for row in tqdm(train_snli):\n",
    "    label_id = label2int[row[3]]\n",
    "    train_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "for row in tqdm(dev_snli):\n",
    "    label_id = label2int[row[3]]\n",
    "    dev_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "for row in tqdm(test_snli):\n",
    "    label_id = label2int[row[3]]\n",
    "    test_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 64\n",
    "\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "word_embedding_model = models.BERT(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_data_nli = SentencesDataset(train_nli_samples, model=model)\n",
    "train_dataloader_nli = DataLoader(train_data_nli, shuffle=True, batch_size=batch_size)\n",
    "train_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
    "\n",
    "\n",
    "dev_data_nli = SentencesDataset(dev_nli_samples, model=model)\n",
    "dev_dataloader_nli = DataLoader(dev_data_nli, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_data_nli = SentencesDataset(test_nli_samples, model=model)\n",
    "test_dataloader_nli = DataLoader(test_data_nli, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_evaluator = LabelAccuracyEvaluator(test_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset and initial model\n",
    "model_name = 'bert-base-uncased'\n",
    "model_save_path = 'output/snli_'+model_name+'-full_iteration_'\n",
    "\n",
    "num_epochs = 1\n",
    "warmup_steps = math.ceil(len(train_data_nli) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "train_objectives = [(train_dataloader_nli, train_loss_nli)]\n",
    "\n",
    "validation_performance = []\n",
    "test_performance = []\n",
    "\n",
    "test_evaluator = LabelAccuracyEvaluator(test_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "dev_evaluator = LabelAccuracyEvaluator(dev_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "\n",
    "validation_performance.append(model.evaluate(dev_evaluator))\n",
    "test_performance.append(model.evaluate(test_evaluator))\n",
    "print(f'Iteration - {0} ...')\n",
    "print(f'Validation performance - {validation_performance[-1]} ...')\n",
    "print(f'Test performance - {test_performance[-1]} ...')\n",
    "\n",
    "for i in range(3):\n",
    "    model.fit(train_objectives=train_objectives, output_path=model_save_path+str(i+1))\n",
    "    validation_performance.append(model.evaluate(dev_evaluator))\n",
    "    test_performance.append(model.evaluate(test_evaluator))\n",
    "    print(f'Iteration - {i+1} ...')\n",
    "    print(f'Validation performance - {validation_performance[-1]} ...')\n",
    "    print(f'Test performance - {test_performance[-1]} ...')\n",
    "    model.save(model_save_path+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same experiment with trancated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snli = _create_examples_snli(_read_tsv('../SemBERT/glue_data/SNLI/train_filtered.tsv'), 'train_s')\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_nli_samples = []\n",
    "dev_nli_samples = []\n",
    "test_nli_samples = []\n",
    "\n",
    "for row in tqdm(train_snli):\n",
    "    label_id = label2int[row[3]]\n",
    "    train_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "for row in tqdm(dev_snli):\n",
    "    label_id = label2int[row[3]]\n",
    "    dev_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "for row in tqdm(test_snli):\n",
    "    label_id = label2int[row[3]]\n",
    "    test_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "    \n",
    "    \n",
    "# Read the dataset\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 64\n",
    "\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "word_embedding_model = models.BERT(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_data_nli = SentencesDataset(train_nli_samples, model=model)\n",
    "train_dataloader_nli = DataLoader(train_data_nli, shuffle=True, batch_size=batch_size)\n",
    "train_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
    "\n",
    "dev_data_nli = SentencesDataset(dev_nli_samples, model=model)\n",
    "dev_dataloader_nli = DataLoader(dev_data_nli, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_data_nli = SentencesDataset(test_nli_samples, model=model)\n",
    "test_dataloader_nli = DataLoader(test_data_nli, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_evaluator = LabelAccuracyEvaluator(test_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "\n",
    "\n",
    "# Read the dataset and initial model\n",
    "model_name = 'bert-base-uncased'\n",
    "model_save_path = 'output/snli_'+model_name+'-truncated_iteration_'\n",
    "\n",
    "num_epochs = 1\n",
    "warmup_steps = math.ceil(len(train_data_nli) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "train_objectives = [(train_dataloader_nli, train_loss_nli)]\n",
    "\n",
    "test_evaluator = LabelAccuracyEvaluator(test_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "dev_evaluator = LabelAccuracyEvaluator(dev_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "\n",
    "validation_performance.append(model.evaluate(dev_evaluator))\n",
    "test_performance.append(model.evaluate(test_evaluator))\n",
    "print(f'Iteration - {0} ...')\n",
    "print(f'Validation performance - {validation_performance[-1]} ...')\n",
    "print(f'Test performance - {test_performance[-1]} ...')\n",
    "\n",
    "for i in range(3):\n",
    "    model.fit(train_objectives=train_objectives, output_path=model_save_path+str(i+1))\n",
    "    validation_performance.append(model.evaluate(dev_evaluator))\n",
    "    test_performance.append(model.evaluate(test_evaluator))\n",
    "    print(f'Iteration - {i+1} ...')\n",
    "    print(f'Validation performance - {validation_performance[-1]} ...')\n",
    "    print(f'Test performance - {test_performance[-1]} ...')\n",
    "    model.save(model_save_path+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a = {'validation_performance': validation_performance,\n",
    "     'test_performance': test_performance}\n",
    "\n",
    "with open('results.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('results.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with effitiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "embeddings = []\n",
    "for sam in tqdm(test_nli_samples):\n",
    "    embeddings.append(train_loss_nli.model.encode(sam.texts, show_progress_bar = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [] \n",
    "for embs in tqdm(embeddings):\n",
    "    d = np.hstack([embs[0], embs[1], np.abs(embs[0] - embs[1])])\n",
    "    preds.append(np.argmax(train_loss_nli.classifier(torch.Tensor(d)).detach().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
